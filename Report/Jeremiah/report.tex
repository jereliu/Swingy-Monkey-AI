\documentclass[11pt]{article}

%formating author affiliation
\usepackage{authblk}
\author[1]{(Jeremiah) Zhe Liu}
\author[2]{(Vivian) Wenwan Yang}
\author[1]{Jing Wen}
\affil[1]{Department of Biostatistics, Harvard School of Public Health}
\affil[2]{Department of Computational Science and Engineering, SEAS}

% change document font family to Palatino, and code font to Courier
\usepackage{mathpazo} % add possibly `sc` and `osf` options
\usepackage{eulervm}
\usepackage{courier}
%allow formula formatting

%identation in nested enumerates
\usepackage[shortlabels]{enumitem}
\setlist[enumerate,1]{leftmargin=1cm} % level 1 list
\setlist[enumerate,2]{leftmargin=2cm} % level 2 list

%flush align equations to left, this also loads amsmath 
%\usepackage[fleqn]{mathtools}
\usepackage{mathtools}
\usepackage{amsthm}
\DeclareMathAlphabet\mathbfcal{OMS}{cmsy}{b}{n}
\usepackage{comment}

%declare math symbolz
%# inner product
\DeclarePairedDelimiter{\inner}{\langle}{\rangle}

%declare argmin
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}

%declare checkmark
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

%title positon
\usepackage{titling} %fix title
\setlength{\droptitle}{-6em}   % Move up the title 

%change section title font size
\usepackage{titlesec} 
\titleformat{\section}
  {\normalfont\fontsize{12}{15}}{\thesection}{1em}{}
\titleformat{\subsection}
  {\normalfont\fontsize{12}{13}}{\thesubsection}{1em}{}
\titleformat{\subsubsection}
  {\normalfont\fontsize{12}{13}}{\thesubsubsection}{1em}{}

%overwrite bfseries to allow formula in section title  
\def\bfseries{\fontseries \bfdefault \selectfont \boldmath}

% change page margin
\usepackage[margin=0.8 in]{geometry} 

%disable indentation
\setlength\parindent{0pt}

%allow inserting multiple graphs
\usepackage{graphicx}
\usepackage[skip=1pt]{subcaption}
\usepackage[justification=centering,font=small]{caption}
\newcommand{\indep}{\rotatebox[origin=c]{90}{$\models$}}%indep sign

%allow code chunks
\usepackage{listings}
%\lstset{basicstyle=\footnotesize\ttfamily,breaklines=true}
\lstset{basicstyle=\footnotesize\ttfamily,breaklines=true}
\lstset{frame=lrbt,xleftmargin=\fboxsep, xrightmargin=-\fboxsep}
\lstset{language=R, commentstyle=\bfseries, 
keywordstyle=\ttfamily} %R-related formatting
\lstset{escapeinside={<@}{@>}}

%allow merged cell in tables
\usepackage{multirow}

%allow http links
\usepackage{hyperref}

%allow different font colors
\usepackage{xcolor}

%Thm and Def environment
\theoremstyle{definition}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}

\newenvironment{definition2}[1][Definition]{\begin{trivlist} %def without index
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

\newenvironment{example}[1][Example]{\begin{trivlist} %def without index
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}


%macros from Bob Gray
\usepackage{"./macro/GrandMacros"}
\usepackage{"./macro/Macro_BIO235"}

\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%% TItle page with contents %%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{\textbf{CS 181 Machine Learning}\\ 
\textbf{Practical 4 Report, Team \textit{la Derni\`{e}re Dame M}}}

\pretitle{\begin{centering}\Large}
\posttitle{\par\end{centering}}

\date{\today}
\vspace{-10em}
\maketitle
\vspace{-2em}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%% Formal Sections %%%%% %%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{\textbf{Problem Description}}

Set in a \textit{Flappy Bird}-type game \textit{Swingy Monkey}, our current learning task is to estimate the optimal policy function $\pi: \Ssc \rightarrow \Asc$ such that the expectation of reward function $R: \Ssc \times \Asc \rightarrow \Rsc$ is maximized, i.e. if define a stochastic process of game state $\{s_t\}$ with unknown transition probability $P(s_{t+1} \big| s_1, \dots, s_t, a_1, \dots, a_t)$, we aim to identify a $\pi^*$ such that

\begin{align*}
\pi^* = arg\max_\pi E \Big( \sum_{s \in \bp} R(s, \pi(s)) \Big| \bp \Big)
\end{align*}

where $\bp$ a sample path of $\{S_t\}$.\\

In current setting, the state and action spaces are defined as:
\begin{align*}
\Ssc & =
\begin{bmatrix}
Tree_{\mbox{dist}} & Tree_{\mbox{top}} & Tree_{\mbox{bot}} &
Monkey_{\mbox{vel}} & Monkey_{\mbox{top}} & Monkey_{\mbox{bot}}
\end{bmatrix} \subset \mathbb{R}^6\\
\Asc & = \begin{bmatrix} No Jump & Jump \end{bmatrix}
\end{align*}
Note theta $\big[ 
Monkey_{\mbox{top}}, Monkey_{\mbox{bot}}, Tree_{\mbox{top}}, Tree_{\mbox{bot}}  \big]$ are in fact bounded by screen size (600 pxls).\\

The reward function can be partially described as:
\begin{align*}
R: \quad
\begin{bmatrix}
\mbox{{\tt pass\_tree}} \\
\mbox{{\tt hit\_trunk}} \\
\mbox{{\tt hit\_edge}} \\
\mbox{{\tt otherwise}} \\
\end{bmatrix}
\rightarrow
\begin{bmatrix}
1 \\ -5 \\ -10 \\ 0
\end{bmatrix}
\end{align*}

where $\big[ \mbox{{\tt pass\_tree}}, \; \mbox{{\tt hit\_trunk}}, \; \mbox{{\tt hit\_edge}} \big]$ are unknown boolean functions of $s\in \Ssc$.

\newpage
\section{\textbf{Method}}

\subsection{\textbf{Rationale on Model Choice}}
In the previous section we identified below characteristics of the task at hand:
\begin{enumerate}
\item [(1)] Available Information:
\begin{enumerate}
\item Known $\Ssc$, $\Asc$ spaces. 
\item Unknown transition probability $P(s_{t+1} \big| \{s_i\}_{i=0}^{t}, \{a_i\}_{i=1}^{t})$ and unknown reward function $R: \Ssc \times \Asc \rightarrow \Rsc$
\end{enumerate}
\item [(2)] $|\Asc| = 2$, while $\Ssc \subset \mathbb{R}^6$ is continous with $|\Ssc| = \infty$. 
\item [(3)] Outcome metric: 
$E \Big( \sum_{s \in \bp} R(s, \pi(s)) \Big| \bp \Big)$ the expected number of total reward in each play.
\end{enumerate}

If we are willing to assume Markovian property for the process $\{s_t\}$, i.e. $P(s_{t+1} \big| \{s_i\}_{i=0}^{t}, \{a_i\}_{i=1}^{t}) = P(s_{t+1} \big| s_t, a_t) $, such that the transition information can be described by a transition matrix $\bP$, the availabile information listed in $(1)$ put us into a Reinforcement Learning setting. Furthemore, based on information from $(2)$, we are relunctant to deploy model-based approach due to the lareg $\Ssc \times \Asc$ space. More specifically, since the model-based approach requires reasonably accurate estimation of $\bP$ (a $|\Ssc| \times |\Ssc| \times 2$ matrix), we need to visit each nontrivial position of $\bP$  sufficiently often in order to achieve reliable $\hat{P}(s'|s, a)$ estimates. Such computation, even after the discreting of the state space, is intractable in terms of both complexity and storage. Although the estimation of $\bP$ and $\bQ$ can be simplified by assuming $P(s'|s, a) = f(s', s, a)$ and $Q(s, a) = g(s, a)$ and model $f(.), g(.)$ using flexible, kernel-based methods, we decided not to add this extra layer of complexity in order to avoid skewed $\bP$ and $\bQ$ estimates due to wrong functional assumption.

Based on above consideration, we focused on Q-learning in our implementation, where the estimation task is greatly simplified by considering only the $\bQ$ matrix. Specifically, by expanding the Bellman equations, we have: 
\begin{align*}
Q(s,a) &= R(s,a)+\gamma\sum P(s'|s,a) max_{a'\in A}Q(s',a')\\
&= R(s,a)+\gamma E_{s'}[max_{a'\in A}Q(s',a')]\\
&=E_{s'}[R(s,a)+\gamma max_{a'\in A}Q(s',a')]
\end{align*}
and estimation may proceed through Stochastic Gradient Descent/Temporal Difference Learning, where $\hat{Q}_{\mbox{target}} = r_{s, a}^{obs} +\gamma max_{a'\in A}Q^{old}(s',a')$. We thus have below updating algorithm:
$$Q(s,a)^{new} \leftarrow 
Q(s,a)^{old} + \alpha [r + \gamma  max_a' Q(s',a')^{old} - Q(s,a)^{old} ]$$

where $\gamma \in (0, 1)$ denotes the discount factor, and $\alpha \in (0, 1)$ denotes the learning rate.

\subsubsection{\textbf{Representing $\Ssc$ in low-dimension space}}

Due to the continous and high-dimensional nature of original $\Ssc$, it is essential to identify $\Ssc_p$ a minimal-information-loss projection of $\Ssc$ in discrete, lower dimensional space. Specifically, we impose below criteria for our projection $s_p \in \Ssc_p$:
\begin{enumerate}
\item $\Ssc_p$ contains maximal possible information from $\Ssc$ for the purpose of optimizing $\bQ$, \\ i.e. $\inf |Q(s, a) - \hat{Q}(s_p, a)| < \epsilon$
\item $\Ssc_p$ reasonably satisfies markov assumption, i.e. $P(s_{p,t+1}|s_{p,t}, a_{t}) = P(s_{p, t+1}| \{s_{p, i}\}_{i=1}^t, \{a_{p, i}\}_{i=1}^t )$
\end{enumerate}

Although above criteria are difficult to verify theoretically, they did provide some intuitive guidance in terms of selecting minimal-information-reduction transformation of $\Ssc$.  




\subsubsection{\textbf{Exploration/Exploitation Parameters}}
The learning rate $\alpha$ determining how much of an effect the new sample has on the current estimate. If $\alpha$ is large, we will adjust quickly but may not converge. What we did is to decrease $\alpha$ gradually as the number of samples of $Q(s,a)$ increases.

Learning rate

$\epsilon$-greedy

{\color{red}
\subsection{\textbf{Exploration vs Exploitation}}
The monkey has to determine what action to take even while it is learning. It receives rewards and punishments even as it is learning. The monkey has to spend time to learn the pipes, but we will expect the monkey to start being productive before the monkey has learned everything there is to know about the pipe. The monkey needs to decide what to do considering both the effect of its action on its immediate rewards and future state, and the need to learn for the future. This issue is known as the exploration-exploitation tradeoff. Q-learning has the following two theoretical properties:
\begin{enumerate}
	\item [i] If every state-action pair(s,a) is visited an unbounded number of times and the learning rate $\alpha$ is "eventually small enough" then the Q-values converge to the limit.
	\item [ii] If we exploit the Q-values in the limit, then the policy converges to the optimal policy in the limit.
\end{enumerate}
In order to achieve these two requirements, we define the distinct learning rate for each state/action pair and have that rate be $\alpha_k(s,a)=1/k$ where k is the number of times action $\alpha$ has been taken from state s. We adopt the $"\varepsilon-greedy"$ policy that the optimal action is taken with probability $1-\varepsilon$, but with probability $\varepsilon$, a uniformly random action is taken to induce exploration.We took $\varepsilon=1/t$, where t is the number of time periods that the monkey has experience.
}



\subsection{\textbf{Performance Evaluation}}


\section{\textbf{Result}}

\subsection{\textbf{State Exploration}}

\subsection{\textbf{Convergence Behavior}}


\section{\textbf{Discussion \& Possible Directions}}


\newpage
\section*{\textbf{Reference}}
\begin{enumerate}
\item \label{ref:handbook}
Ricci F, Rokach L, Shapira B et al. (2010) \textbf{Recommender Systems Handbook}. \textit{Springer}. 
\item \label{ref:MFieee}
Koren Y, Bell R, Volinsky C. (2009) \textbf{Matrix factorization techniques for recommender systems}. \textit{IEEE Computer} Aug 2009, 42-49. 
\item \label{ref:WLA}
Srebro N,  Jaakkola T.(2003) \textbf{Weighted low-rank approximations}. \textit{Proceedings of the Twentieth International Conference} 720–727.
\item \label{ref:PMF}
R Salakhutdinov, A Mnih. (2008) \textbf{Probabilistic Matrix Factorization}. \textit{Advances in Neural Information Processing Systems} Vol. 20

\item \label{ref:implicit}
Koren, Y. (2008) \textbf{Factorization Meets the Neighborhood: a Multifaceted Collaborative Filtering Model}, \textit{Proc. 14th ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining}.

\end{enumerate}

\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
